<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>python - 标签 - mine blog</title>
        <link>http://101.35.190.67/tags/python/</link>
        <description>python - 标签 - mine blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>soufj17857@gmail.com (bone)</managingEditor>
            <webMaster>soufj17857@gmail.com (bone)</webMaster><lastBuildDate>Sun, 09 Feb 2020 15:30:49 &#43;0800</lastBuildDate><atom:link href="http://101.35.190.67/tags/python/" rel="self" type="application/rss+xml" /><item>
    <title>Mac设置python环境</title>
    <link>http://101.35.190.67/mac%E8%AE%BE%E7%BD%AEpython%E7%8E%AF%E5%A2%83/</link>
    <pubDate>Sun, 09 Feb 2020 15:30:49 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/mac%E8%AE%BE%E7%BD%AEpython%E7%8E%AF%E5%A2%83/</guid>
    <description><![CDATA[1.包安装 1 2 pip3 install virtualenv pip3 install virtualenvwrapper 2.创建目录存放虚拟环境 1 mkdir ~/.virtualenvs 3.修改zsh 通过which virtualenv和which virtualenvwr]]></description>
</item><item>
    <title>pymysql 读取大数据内存卡死的解决方案</title>
    <link>http://101.35.190.67/pymysql-%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%86%85%E5%AD%98%E5%8D%A1%E6%AD%BB%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link>
    <pubDate>Wed, 20 Nov 2019 09:58:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/pymysql-%E8%AF%BB%E5%8F%96%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%86%85%E5%AD%98%E5%8D%A1%E6%AD%BB%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid>
    <description><![CDATA[背景：目前表中只有5G(后期持续增长)，但是其中一个字段(以下称为detail字段)存了2M(不一定2M，部分为0，平均下来就是2M)，字段]]></description>
</item><item>
    <title>一次使用scrapy的问题记录</title>
    <link>http://101.35.190.67/%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8scrapy%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</link>
    <pubDate>Thu, 12 Sep 2019 14:01:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8scrapy%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</guid>
    <description><![CDATA[前景描述: 需要获取某APP的全国订单量，及抢单量。由于没有全国的选项所以只能分别对每一个城市进行订单的遍历。爬虫每天运行一次，一次获取48小]]></description>
</item><item>
    <title>Django-查询优化</title>
    <link>http://101.35.190.67/django-%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/</link>
    <pubDate>Sat, 07 Sep 2019 13:27:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/django-%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/</guid>
    <description><![CDATA[表数据： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from django.db import models class Province(models.Model): name = models.CharField(max_length=10) def __str__(self): return self.name class City(models.Model): name = models.CharField(max_length=5) province = models.ForeignKey(Province) def __str__(self): return self.name class Person(models.Model): firstname = models.CharField(max_length=10) lastname = models.CharField(max_length=10) visitation = models.ManyToManyField(City, related_name = &#34;visitor&#34;) hometown = models.ForeignKey(City, related_name = &#34;birth&#34;)]]></description>
</item><item>
    <title>scrapy中间件中发送邮件</title>
    <link>http://101.35.190.67/scrapy%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%B8%AD%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</link>
    <pubDate>Mon, 05 Aug 2019 06:36:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/scrapy%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%B8%AD%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</guid>
    <description><![CDATA[背景介绍：之前写过通过通过scrapy的扩展发送邮件，在爬虫关闭的时候发送邮件。那个时候有个问题就是MailSender对象需要return]]></description>
</item><item>
    <title>Python重试模块retrying</title>
    <link>http://101.35.190.67/python%E9%87%8D%E8%AF%95%E6%A8%A1%E5%9D%97retrying/</link>
    <pubDate>Thu, 18 Jul 2019 06:45:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/python%E9%87%8D%E8%AF%95%E6%A8%A1%E5%9D%97retrying/</guid>
    <description><![CDATA[Python重试模块retrying 工作中经常碰到的问题就是，某个方法出现了异常，重试几次。循环重复一个方法是很常见的。比如爬虫中的获取代理]]></description>
</item><item>
    <title>python通过TimedRotatingFileHandler按时间切割日志</title>
    <link>http://101.35.190.67/python%E9%80%9A%E8%BF%87timedrotatingfilehandler%E6%8C%89%E6%97%B6%E9%97%B4%E5%88%87%E5%89%B2%E6%97%A5%E5%BF%97/</link>
    <pubDate>Wed, 17 Jul 2019 06:47:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/python%E9%80%9A%E8%BF%87timedrotatingfilehandler%E6%8C%89%E6%97%B6%E9%97%B4%E5%88%87%E5%89%B2%E6%97%A5%E5%BF%97/</guid>
    <description><![CDATA[通过TimedRotatingFileHandler按时间切割日志 线上跑了一个定时脚本，每天生成的日志文件都写在了一个文件中。但是日志信息不]]></description>
</item><item>
    <title>python定时任务APScheduler</title>
    <link>http://101.35.190.67/python%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1apscheduler/</link>
    <pubDate>Mon, 15 Jul 2019 01:20:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/python%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1apscheduler/</guid>
    <description><![CDATA[APScheduler定时任务 APScheduler 支持三种调度任务：固定时间间隔，固定时间点（日期），Linux 下的 Crontab 命令。同时，它还支持异步执行、后台执行调]]></description>
</item><item>
    <title>清理特殊文本</title>
    <link>http://101.35.190.67/%E6%B8%85%E7%90%86%E7%89%B9%E6%AE%8A%E6%96%87%E6%9C%AC/</link>
    <pubDate>Mon, 08 Jul 2019 11:22:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/%E6%B8%85%E7%90%86%E7%89%B9%E6%AE%8A%E6%96%87%E6%9C%AC/</guid>
    <description><![CDATA[清理特殊文本 unicodedata.normalize(form, unistr) 把一串UNICODE字符串转换为普通格式的字符串，具体格式支持NFC、NFKC、NFD和NFKD格式。 Unicode标准定义]]></description>
</item><item>
    <title>scrapy发送邮件</title>
    <link>http://101.35.190.67/scrapy%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</link>
    <pubDate>Mon, 29 Apr 2019 07:24:00 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://101.35.190.67/scrapy%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</guid>
    <description><![CDATA[scrapy发送邮件 应用场景：在爬虫关闭或者爬虫空闲时可以通过发送邮件的提醒。 通过twisted的非阻塞IO实现,可以直接写在spider中]]></description>
</item></channel>
</rss>
